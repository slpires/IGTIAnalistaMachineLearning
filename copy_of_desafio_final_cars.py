# -*- coding: utf-8 -*-
"""Copy of desafio_final_cars.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-NEYHhJelWq3Kt0-xj3DbYBFKfX1EW94

# Desafio Final - IGTI - Cars
- Autor: Rodrigo de Lima Oliveira
- LinkedIn: https://www.linkedin.com/in/rodrigolima82/

![title](https://github.com/RodrigoLima82/bootcamp_igti_machine_learning/blob/master/Modulo_05/cars_image.png?raw=1)

## Carregando as bibliotecas
"""

# Instalando bibliotecas no Google Colab
!pip install -q scikit-plot
!pip install -q watermark

# Imports de avisos
import sys
import warnings
import matplotlib.cbook
warnings.simplefilter("ignore")
warnings.simplefilter(action='ignore', category=FutureWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=matplotlib.cbook.mplDeprecation)

# Imports para manipulação de dados
import numpy as np
import pandas as pd
import scipy
import statsmodels.api as sm
import math
import itertools

# Imports para visualização de dados
import matplotlib.pyplot as plt
import matplotlib as m
import matplotlib.dates as mdates
from matplotlib.ticker import MaxNLocator
import seaborn as sns
import plotly as py
import plotly.express as px
import plotly.graph_objs as go 
from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot

from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"

pd.options.display.max_columns = 2000
pd.options.display.max_rows = 2000

# Carregando bibliotecas do pacote sklearn
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, classification_report, confusion_matrix
from scikitplot.metrics import plot_confusion_matrix, plot_roc

# Commented out IPython magic to ensure Python compatibility.
# Versões dos pacotes usados neste jupyter notebook
# %reload_ext watermark
# %watermark -a "Desafio Final - Rodrigo" --iversions

"""# Criando funções que serão utilizadas neste notebook"""

# Funcao para verificar valores missing no dataset
def check_missing(df, display = 10):
    temp_df = df.copy()
    df_nan = (temp_df.isnull().sum() / len(temp_df)) * 100
    missing_data = pd.DataFrame({'Missing n': temp_df.isnull().sum(),'% Missing' :df_nan})
    if missing_data['Missing n'].sum() == 0:
        return print('Ótimo! Não há mais valores faltantes neste dataset.')
    else:
        return missing_data.sort_values('% Missing', ascending = False).head(display)
        
# Funcao para criar um grafico de distribuicao pra cada feature do dataset
def plot_distribution(dataset, cols=5, width=20, height=25, hspace=0.4, wspace=0.5):
    fig = plt.figure(figsize=(width, height))
    fig.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=wspace, hspace=hspace)
    rows = math.ceil(float(dataset.shape[1]) / cols)
    for i, column in enumerate(dataset.columns):
        ax = fig.add_subplot(rows, cols, i + 1)
        ax.set_title(column)
        if dataset.dtypes[column] == np.object:
            g = sns.countplot(y=column, 
                              data=dataset,
                              order=dataset[column].value_counts().index[:10])
            substrings = [s.get_text()[:20] for s in g.get_yticklabels()]
            g.set(yticklabels=substrings)
            plt.xticks(rotation=25)
        else:
            g = sns.distplot(dataset[column])
            plt.xticks(rotation=25)        
            
# Funcao para calcular o coeficiente de correlacao entre duas variaveis
def rsquared(x, y):
    slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(x, y)
    return r_value**2     

# Funcao para exibir o gráfico de matriz de confusao
def plot_confusion_matrix(cm,target_names,title='Confusion matrix',cmap=None,normalize=True):

    accuracy = np.trace(cm) / float(np.sum(cm))
    misclass = 1 - accuracy

    if cmap is None:
        cmap = plt.get_cmap('Blues')

    plt.figure(figsize=(8, 6))
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()

    if target_names is not None:
        tick_marks = np.arange(len(target_names))
        plt.xticks(tick_marks, target_names, rotation=45)
        plt.yticks(tick_marks, target_names)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    thresh = cm.max() / 1.5 if normalize else cm.max() / 2
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        if normalize:
            plt.text(j, i, "{:0.4f}".format(cm[i, j]),
                     horizontalalignment="center",
                     color="white" if cm[i, j] > thresh else "black")
        else:
            plt.text(j, i, "{:,}".format(cm[i, j]),
                     horizontalalignment="center",
                     color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))
    plt.show()

"""## Carregar o arquivo CSV"""

# Importando o arquivo 'cars.csv'
from google.colab import files
uploaded = files.upload()

# Fazer a leitura do arquivo com o pandas
train = pd.read_csv('cars.csv')

# Visualizando o shape do dataset (261 linhas x 8 colunas)
print(train.shape)

# Verificando os tipos de dados
train.info()

# Visualizando as primeiras linhas do dataset
train.head()

"""# Analise Exploratória dos Dados"""

train.describe()

# Verificando as colunas com dados missing do dataset
check_missing(train, display = 8)

# Verificando valores únicos
for col in list(train.columns):
    
    # Obtém uma lista de valores únicos
    list_of_unique_values = train[col].unique()
    
    # Se o número de valores exclusivos for menor que 15, imprima os valores. 
    # Caso contrário, imprima o número de valores exclusivos
    if len(list_of_unique_values) < 15:
        print(col + ': ' + str(len(list_of_unique_values)) + ' valores únicos')
        print(list_of_unique_values)
    else:
        print(col + ': ' + str(len(list_of_unique_values)) + ' valores únicos')

"""## Analisando as feature do dataset"""

# Gráfico de distribuição para cada feature do dataset
columns = ['mpg', 'cylinders', 'cubicinches', 'hp', 'weightlbs', 'time-to-60', 'year', 'brand']
plot_distribution(train[columns], cols=3, width=30, height=20, hspace=0.45, wspace=0.5)

"""# Visão geral

> - O conjunto de dados **'cars'** do exemplo contém 261 registros e 8 colunas;
- Analisando os valores faltantes, não foi encontrado nenhum registro, no entanto, caracteres como '?' e espaço não são considerados 'na', por isso, melhor dar uma olhada melhor;
- As features `cubicinches` e `weightlbs` foram classificadas pelo pandas como categóricas, mas tudo indica serem numéricas (vamos avaliar a transformação daqui a pouco);

## Transformação das features `cubicinches` e `weightlbs`
"""

# Defini quais features serao convertidas
cols_to_numeric = ['cubicinches','weightlbs']

# Realiza a conversao usando pd.to_numeric()
train[cols_to_numeric] = train[cols_to_numeric].apply(pd.to_numeric, errors='coerce')

# Verificando se após a transformação encontramos valores missing no dataset
check_missing(train, display = 8)

# Verificando os registros que contem valores nulos
train[train.isnull().any(axis=1)]

"""## Observações

> - Após a transformação das features `cubicinches` e `weightlbs` do tipo string para numerica, houve mudanças nos dados, sendo que agora temos 5 registros com valores nulos nessas duas colunas
- Os índices desses registros são: 14, 33, 40, 172 e 180

## Tratamento de valores missing
"""

# Para corrigir os valores nulos, vamos atribuir a media dos valores das respectivas colunas
train['cubicinches'].fillna(train.cubicinches.mean(), inplace=True)
train['weightlbs'].fillna(train.weightlbs.mean(), inplace=True)

train.describe()

# Verificando os dados da feature 'time-to-60' através de um boxplot
sns.set_style("whitegrid")  
sns.boxplot(y='time-to-60', data=train);

"""## Observações

> - Vamos detalhar essas interpretações:
    - O centro da distribuição é indicado pela linha da mediana, no centro do quadrado (no exemplo é 16).
    - A dispersão é representada pela amplitude do gráfico, que pode ser calculada como máximo valor – mínimo valor. Quanto maior for a amplitude, maior a variação nos dados.
    - O retângulo contém 50% dos valores do conjunto de dados. A posição da linha mediana no retângulo informa sobre a assimetria da distribuição. Uma distribuição simétrica teria a mediana no centro do retângulo. Se a mediana é próxima de Q1, então, os dados são positivamente assimétricos.Se a mediana é próxima de Q3 os dados são negativamente assimétricos. 
    - Os outliers em um box plot aparecem como pontos ou asteriscos fora das “linhas” desenhadas. Perceba que no exemplo, temos um outlier representado pelo ponto no começo e no final do gráfico.

## Visualizando a correlação das features
"""

plt.figure(figsize=(20, 12))

# Selecionando apenas algumas colunas para verificar a correlação
corr = train.corr(method = 'pearson')

# Configurar o heatmap
sns.heatmap(corr, linewidths=.5, vmin=-1, vmax=1, annot=True, cmap="YlGnBu", fmt='.1g')

# Imprimir o grafico
plt.show();

# Visualiza um gráfico entre as variaveis cylinders e mpg e verificar se existe alguma correlação linear
plt.figure(figsize=(18, 8))
sns.regplot(x='cylinders', y='mpg', data=train, color='b', x_jitter=0.2)
plt.xlabel('cylinders')
plt.ylabel('mpg')
plt.title('Relação entre Cylinders e mpg', fontsize=20)
plt.show();

"""### Calculando o coeficiente de correlação de duas formas"""

# Calculando o coeficiente de correlação entre 'cylinders' e 'mpg' através do r2
rsquared(train['cylinders'], train['mpg'])

# Calculando o coeficiente de correlação pelo OLS e exibindo um sumario
model = sm.OLS(train['mpg'], train['cylinders'])
results = model.fit()
results.summary()

"""## Observações

> - De acordo com o gráfico de correlação de Person, temos uma correlação negativa (-0,8) entre as features `cylinders` e `mpg`;
- Isso significa que quanto maior aumenta a quantidade de cilindros, menos é o mpg do carro;
- Isso fica evidente no gráfico regplot apresentado;
- Quando calculamos o coeficiente de correlação, chegamos no valor aproximado de 0.69, como é possível observar nas duas formas propostas acima;

## Analisando a feature `hp`
"""

# Verificando os dados da feature 'hp' através de um boxplot
sns.set_style("whitegrid")  
sns.boxplot(y='hp', data=train);

"""## Observações

> - De acordo com o boxplot da feature `hp` não há evidências de valores outliers;
- De acordo com a posição da linha mediana no retângulo, que nos indica que está mais próxima de Q1, podemos inferir que os dados são positivamente assimétricos;
- Através desse gráfico é possível observar que a mediana está entre 80 e 100 (no describe acima vimos que é 95).

# Padronizando as features do dataset
"""

# Cria o padronizador
scaler = StandardScaler()

# Definindo somente colunas numericas a serem padronizadas
num_cols = train.columns[train.dtypes.apply(lambda c: np.issubdtype(c, np.number))]

# Criar uma copia do dataset original
new_train = train[num_cols]

# Padronizando os dados
new_train[num_cols] = scaler.fit_transform(new_train[num_cols])

# Exibindo os primeiros registros
new_train.head()

# Verificando o maior valor para a feature 'hp'
new_train['hp'].max()

"""## Observações

> - A idéia por trás de StandardScaler é que ele transformará seus dados de tal forma que sua distribuição terá um valor médio 0 e um desvio padrão de 1. Dada a distribuição dos dados, cada valor no conjunto de dados terá o valor médio da amostra. subtraído e, em seguida, dividido pelo desvio padrão de todo o conjunto de dados;
- Analisando a feature `hp`, após a padronização o maior valor é de 3.05

## Aplicando o PCA e analisando a variância
"""

# Criando o objeto PCA com 7 componentes
pca = PCA(n_components=7)

# Realizando o fit com os dados tratados
principalComponents = pca.fit_transform(new_train)

# Salvando em um dataframe
PCA_components = pd.DataFrame(principalComponents)
PCA_components.head()

# Exibindo o valor da variância explicada por cada componente
print(pca.explained_variance_ratio_)

# Plot da variação explicada pelos componentes
features = range(pca.n_components_)

fig, aux = plt.subplots(1,1, figsize=(24, 6))
plt.bar(features, pca.explained_variance_ratio_, color='black')
plt.xlabel('PCA features')
plt.ylabel('variance %')
plt.xticks(features);

"""## Observações

> - A análise dos componentes principais - ACP ou PCA é um método que tem por finalidade básica a análise dos dados visando sua redução, eliminação de sobreposições e a **escolha das formas mais representativas dos dados a partir de combinações lineares das variaveis originais**;
- A PCA consiste em promover uma transformação linear nos dados de modo que os dados resultantes desta transformação tenham suas componentes mais relevantes nas primeiras dimensões, em eixos denominados principais;
- A matriz  de  covariância contém informação sobre as variâncias em todos os eixos onde os dados estão distribuídos
- No caso do nosso exemplo, o valor da variância explicada pela primeira componente principal é de **72,3%**.

# Construindo um modelo K-means
"""

# Inicializando o kmeans
kmeans = KMeans(n_clusters=3, random_state=42)

# Treinar o modelo utilizando apenas os tres primeiros componentes principais
kmeans.fit(PCA_components.iloc[:,:3])

# Realizando as previsoes dos cluster
X_clustered = kmeans.predict(PCA_components.iloc[:,:3])

# Definindo um mapa de cor para cada cluster
color_map = {0:'r', 1: 'g', 2: 'b'}
label_color = [color_map[l] for l in X_clustered]

# Definindo os centroides
centers = np.array(kmeans.cluster_centers_)

# Exibindo um grafico scatter
fig, aux = plt.subplots(1,1, figsize=(18, 8))
plt.title('Kmeans com centróides', fontsize=20)
plt.scatter(principalComponents[:,0],principalComponents[:,1], c= label_color, alpha=0.5) 
plt.scatter(centers[:,0], centers[:,1], marker="x", color='black', s=500)
plt.show();

# Criando um dataframe do nosso PCA
df = pd.DataFrame(PCA_components)

# Selecionando somente os 3 primeiros componentes
df = df[[0,1,2]]
df['cluster'] = X_clustered

# Visualizando nossos clusters com os dados do PCA
sns.pairplot(df, hue='cluster', palette= 'Dark2', diag_kind='kde', size=3);

# Verificando a quantidade em cada um dos clusters
print(df['cluster'].value_counts())

# Exibindo em um gráfico
df['cluster'].value_counts().plot(kind = 'bar')
plt.ylabel('Count');

"""## Observações

> - Utilizando os 3 primeiros componentes principais para construir o K-means com um número de 3 clusters, podemos verificar que:
    - Os centroides, utilizando apenas as 3 componentes principais, possuem 3 dimensões;
    - Existem 3 centroides após a aplicação da clusterização;
    - Cada um dos clusters possui características próprias.

# Criação e Validação dos Modelos de Machine Learning

### Preparando os dados para treinamento do baseline

- Vamos criar uma nova feature no nosso dataset
- Para isso, preciso buscar o valor original da feature 'mpg' e concatenar com o dataset do PCA
- Como estratégia, farei o merge dos dois datasets pelo index
"""

# Realizando o merge com o dataset original e do pca, gerando um novo dataset
df_final = train.merge(df, left_index=True, right_index=True)

# Cria a nova feature mpg
# Regra: veículos que percorrem mais de 25 milhas com um galão (“mpg” > 25) devem ser considerados eficientes
df_final['new_mpg'] = np.where(df_final['mpg'] > 25, 1, 0)

# Exibir o dataset final
df_final.head()

# Vamos armazenar 'new_mpg' no rótulo (y) e o restante das colunas em X
y = df_final['new_mpg']
X = df_final[['cylinders','cubicinches','hp','weightlbs','time-to-60']]

# Cria o padronizador
scaler = StandardScaler()

# Padronizando os dados
X = scaler.fit_transform(X)

# Split dos dados
X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=.3, random_state=42)

# Verificando o shape apos o split entre feature e target
print(X_train.shape, y_train.shape, X_valid.shape, y_valid.shape)

"""### Realizando o treinamento de todos os modelos"""

# Preparando a lista de modelos
modelos = []
modelos.append(('LogisticRegression', LogisticRegression(random_state=42)))
modelos.append(('DecisionTree', DecisionTreeClassifier(random_state=42)))

# Avaliando cada modelo em um loop
for nome, modelo in modelos:
    
    # realizando o fit do modelo
    modelo.fit(X_train, y_train)
    
    # avaliando o score com dados de validação
    score = modelo.score(X_valid, y_valid)   
    
    # exibindo o resultado
    print('\n')
    msg = "%s: %f" % (nome, score)
    print(msg)

"""## Observações

> - Conseguimos a seguinte acurácia para o baseline:
    - **LogisticRegression = 86%**
    - **DecisionTree = 87%**
- A árvore de decisão e a regressão logística podem ser utilizadas para previsão em regressões;
- A acurácia de ambos os modelos foi superior a 80%;
- Como os dois modelos obtiveram um resultado superior a 80% de acurácia, a escolha sobre qual utilizar deve e pode ser feita a partir de outros critérios, como a complexidade do modelo.

### Realizando o treinamento com **DECISION TREE**
"""

# Treinando o modelo de Arvore de Decisão
model_dt = DecisionTreeClassifier(random_state=42)
model_dt.fit(X_train, y_train)

# Realizar as previsões
previsoes = model_dt.predict(X_valid)

# Fazendo as previsões e construindo o relatório
report = classification_report(y_valid, previsoes)

# Imprimindo o relatório
print(report)

# Permite verificar a acurácia em um formato de tabela
matrix = confusion_matrix(y_valid, previsoes)

# Chamando a função para visualizar a confusion matrix
plot_confusion_matrix(matrix, 
                      target_names = ['0', '1'],
                      normalize    = False,
                      title        = "Confusion Matrix")

"""### Realizando o treinamento com **LOGISTIC REGRESSION**"""

# Treinando o modelo de Regressão Logística
model_lr = LogisticRegression(random_state=42)
model_lr.fit(X_train, y_train)

# Realizar as previsões
previsoes = model_lr.predict(X_valid)

# Fazendo as previsões e construindo o relatório
report = classification_report(y_valid, previsoes)

# Imprimindo o relatório
print(report)

# Permite verificar a acurácia em um formato de tabela
matrix = confusion_matrix(y_valid, previsoes)

# Chamando a função para visualizar a confusion matrix
plot_confusion_matrix(matrix, 
                      target_names = ['0', '1'],
                      normalize    = False,
                      title        = "Confusion Matrix")

"""# FIM"""